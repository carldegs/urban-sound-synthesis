{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of attention_waveform",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49f56ed72e7d4c998a766f0a1d4cb5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_10c2937f0d7946d8b059de94cdd23a06",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3374a6a22d1b463585df3188ec60f6af",
              "IPY_MODEL_c1eb1f3c72034a6fa25a7dba01ba7c75"
            ]
          }
        },
        "10c2937f0d7946d8b059de94cdd23a06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3374a6a22d1b463585df3188ec60f6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f61e16cb0d2745eab1a7c78739758c6d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a9b176035ed4574acc93d1d52646460"
          }
        },
        "c1eb1f3c72034a6fa25a7dba01ba7c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b6dcc8d75654f53a9ad60442470c6ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [10:39&lt;00:00,  1.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72e2221e23ea4717a6d406a7c16e5515"
          }
        },
        "f61e16cb0d2745eab1a7c78739758c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a9b176035ed4574acc93d1d52646460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b6dcc8d75654f53a9ad60442470c6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72e2221e23ea4717a6d406a7c16e5515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjIr7ag14vHR",
        "outputId": "89bdab93-afd4-490a-9c00-d5099f7e56e9"
      },
      "source": [
        "!pip install -U torchtext==0.8.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2oOa1pQRuGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379bb1ed-e762-46ea-ba3a-a97de75d0abb"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "CONTENT_FOLDER = \"/content\"\r\n",
        "DRIVE_FOLDER = \"/content/gdrive/MyDrive/miniproj\"\r\n",
        "DATASET_FOLDER = F'{DRIVE_FOLDER}/UrbanSound8K'\r\n",
        "\r\n",
        "model_save_name = 'classifier.pt'\r\n",
        "MODEL_PATH = F\"{DRIVE_FOLDER}/{model_save_name}\" \r\n",
        "\r\n",
        "DOWNLOAD_DATASET = False\r\n",
        "EXTRACT_DATASET = False\r\n",
        "\r\n",
        "# DATSET_OPTIONS = ['GET_WAVEFORM', 'SAVE_CSV', 'LOAD_CSV', 'SKIP']\r\n",
        "DATASET_OPTION = 'GET_WAVEFORM'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "celH2xOFSPG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ae3e3a-4765-43dd-842d-6f2eeae8445d"
      },
      "source": [
        "import os.path\r\n",
        "import tarfile\r\n",
        "\r\n",
        "%cd /content/gdrive/MyDrive/miniproj\r\n",
        "\r\n",
        "if DOWNLOAD_DATASET:\r\n",
        "    ! wget https://goo.gl/8hY5ER -O dataset.tar.gz\r\n",
        "\r\n",
        "if EXTRACT_DATASET:\r\n",
        "    tar = tarfile.open('dataset.tar.gz', 'r:gz')\r\n",
        "    tar.extractall()\r\n",
        "    tar.close()\r\n",
        "\r\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgVDaoxRf0Q2"
      },
      "source": [
        "import math\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class TransformerModel(nn.Module):\r\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\r\n",
        "        super(TransformerModel, self).__init__()\r\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\r\n",
        "        self.model_type = 'Transformer'\r\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\r\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\r\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\r\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\r\n",
        "        self.ninp = ninp\r\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\r\n",
        "\r\n",
        "        self.init_weights()\r\n",
        "\r\n",
        "    def generate_square_subsequent_mask(self, sz):\r\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\r\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\r\n",
        "        return mask\r\n",
        "\r\n",
        "    def init_weights(self):\r\n",
        "        initrange = 0.1\r\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\r\n",
        "        self.decoder.bias.data.zero_()\r\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\r\n",
        "\r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\r\n",
        "        src = self.pos_encoder(src)\r\n",
        "        output = self.transformer_encoder(src, src_mask)\r\n",
        "        output = self.decoder(output)\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQebIiQbhuLr"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\r\n",
        "        super(PositionalEncoding, self).__init__()\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "\r\n",
        "        pe = torch.zeros(max_len, d_model)\r\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\r\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\r\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\r\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\r\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\r\n",
        "        self.register_buffer('pe', pe)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = x + self.pe[:x.size(0), :]\r\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9WMmBRCq0TG"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=\"1\"\r\n",
        "\r\n",
        "import io\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "from torchtext.vocab import build_vocab_from_iterator\r\n",
        "import numpy as np\r\n",
        "import librosa\r\n",
        "import librosa.display\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "\r\n",
        "SOF = 299.\r\n",
        "EOF = 99.\r\n",
        "\r\n",
        "def get_waveform(idx, rows):\r\n",
        "    row = rows[idx]\r\n",
        "    filename = row[0]\r\n",
        "    fold = row[1]\r\n",
        "    wav, sr = librosa.load(f'{DATASET_FOLDER}/audio/fold{fold}/{filename}', sr=8000)\r\n",
        "    wav = np.around(wav, 4)\r\n",
        "    wav = np.append(wav, EOF)\r\n",
        "    wav = np.insert(wav, 0, SOF)\r\n",
        "\r\n",
        "    return wav\r\n",
        "\r\n",
        "def get_audio_data_from_csv(category='dog_bark'):\r\n",
        "    df = pd.read_csv(F'{DATASET_FOLDER}/metadata/UrbanSound8K.csv')\r\n",
        "    groupedData = df[['slice_file_name', 'fold', 'class']].groupby('class').apply(np.array)\r\n",
        "    rows = groupedData[category]\r\n",
        "\r\n",
        "    audio_data = []\r\n",
        "    unique_wav_data = []\r\n",
        "\r\n",
        "    for idx in tqdm(range(int(len(rows)))):\r\n",
        "        audio_wav_sr_data = get_waveform(idx, rows)\r\n",
        "\r\n",
        "        audio_data.append(audio_wav_sr_data)\r\n",
        "\r\n",
        "    return audio_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF8qK50JzCvE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "49f56ed72e7d4c998a766f0a1d4cb5b1",
            "10c2937f0d7946d8b059de94cdd23a06",
            "3374a6a22d1b463585df3188ec60f6af",
            "c1eb1f3c72034a6fa25a7dba01ba7c75",
            "f61e16cb0d2745eab1a7c78739758c6d",
            "0a9b176035ed4574acc93d1d52646460",
            "5b6dcc8d75654f53a9ad60442470c6ba",
            "72e2221e23ea4717a6d406a7c16e5515"
          ]
        },
        "outputId": "4372f91f-f4b4-434e-f027-2fb1a1c54af4"
      },
      "source": [
        "# Optional: save data to csv\r\n",
        "import csv\r\n",
        "\r\n",
        "if DATASET_OPTION == 'GET_WAVEFORM' or DATASET_OPTION == 'SAVE_CSV':\r\n",
        "    data = get_audio_data_from_csv()\r\n",
        "\r\n",
        "    if DATASET_OPTION == 'SAVE_CSV':\r\n",
        "        with open(F\"{DRIVE_FOLDER}/waveforms.csv\",\"w+\") as my_csv:\r\n",
        "            csvWriter = csv.writer(my_csv, delimiter=',')\r\n",
        "            csvWriter.writerows(data)\r\n",
        "elif DATASET_OPTION == 'LOAD_CSV':\r\n",
        "    # TODO: CHECK IF CORRECT\r\n",
        "    data2 = pd.read_csv(F'{DRIVE_FOLDER}/waveforms.csv')\r\n",
        "\r\n",
        "if DATASET_OPTION != 'SKIP':\r\n",
        "    b = int(len(data)*0.8)\r\n",
        "    train_data = data[0:b]\r\n",
        "    val_data = data[b+1:len(data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsKIlwoRvN-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "042dd801-6595-4b92-83a4-f14e55e31513"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "vocab = build_vocab_from_iterator(iter(data))\r\n",
        "\r\n",
        "def waveform_detokenizer(iter):\r\n",
        "    arr = [np.round((np.array(item) / 10000) - 1, 4) for item in iter]\r\n",
        "    return arr\r\n",
        "\r\n",
        "def data_process(iter):\r\n",
        "    data = [torch.tensor([vocab[token] for token in row], dtype=torch.long) for row in iter]\r\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\r\n",
        "\r\n",
        "def batchify(data, bsz):\r\n",
        "    nbatch = data.size(0) // bsz\r\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\r\n",
        "    data = data.view(bsz, -1).t().contiguous()\r\n",
        "\r\n",
        "    return data.to(device)\r\n",
        "\r\n",
        "def get_batch(source, i):\r\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\r\n",
        "    data = source[i:i+seq_len]\r\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\r\n",
        "    return data, target\r\n",
        "\r\n",
        "train_data = data_process(iter(train_data))\r\n",
        "val_data = data_process(iter(val_data))\r\n",
        "\r\n",
        "bptt = 35\r\n",
        "batch_size = 32\r\n",
        "eval_batch_size = 16\r\n",
        "train_data = batchify(train_data, batch_size)\r\n",
        "val_data = batchify(val_data, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt5_VRWA4-EX"
      },
      "source": [
        "import os.path\r\n",
        "\r\n",
        "ntokens = len(vocab.stoi) # the size of vocabulary\r\n",
        "emsize = 200 # embedding dimension\r\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\r\n",
        "nlayers = 32 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\r\n",
        "nhead = 2 # the number of heads in the multiheadattention models\r\n",
        "dropout = 0.2 # the dropout value\r\n",
        "\r\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\r\n",
        "\r\n",
        "if os.path.isfile(F\"{DRIVE_FOLDER}/{model_save_name}\" ):\r\n",
        "    print('loading checkpoint...')\r\n",
        "    checkpoint = torch.load(MODEL_PATH)\r\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])#.to(device)\r\n",
        "    model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2y0gjSP1u47"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "lr = 0.1 # learning rate\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\r\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.1)\r\n",
        "\r\n",
        "if os.path.isfile(F\"{DRIVE_FOLDER}/{model_save_name}\" ):\r\n",
        "    print('loading checkpoint...')\r\n",
        "    checkpoint = torch.load(MODEL_PATH)\r\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "\r\n",
        "import time\r\n",
        "def train():\r\n",
        "    model.train() # Turn on the train mode\r\n",
        "    total_loss = 0.\r\n",
        "    start_time = time.time()\r\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\r\n",
        "    # for batch, i in enumerate(tqdm(range(0, train_data.size(0) - 1, bptt))):\r\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\r\n",
        "        data, targets = get_batch(train_data, i)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        if data.size(0) != bptt:\r\n",
        "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\r\n",
        "        output = model(data, src_mask)\r\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\r\n",
        "        loss.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        total_loss += loss.item()\r\n",
        "        log_interval = 3000\r\n",
        "        if batch % log_interval == 0 and batch > 0:\r\n",
        "            cur_loss = total_loss / log_interval\r\n",
        "            elapsed = time.time() - start_time\r\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\r\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\r\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\r\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\r\n",
        "                    elapsed * 1000 / log_interval,\r\n",
        "                    cur_loss, math.exp(cur_loss)))\r\n",
        "            total_loss = 0\r\n",
        "            start_time = time.time()\r\n",
        "\r\n",
        "def evaluate(eval_model, data_source):\r\n",
        "    eval_model.eval()\r\n",
        "    total_loss = 0.\r\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\r\n",
        "    with torch.no_grad():\r\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\r\n",
        "            data, targets = get_batch(data_source, i)\r\n",
        "            if data.size(0) != bptt:\r\n",
        "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\r\n",
        "            output = eval_model(data, src_mask)\r\n",
        "            output_flat = output.view(-1, ntokens)\r\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\r\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PjXR-4A4LXV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "12fa2834-2f9a-4ec6-fa31-2bc880f8c5f4"
      },
      "source": [
        "best_val_loss = float(\"inf\")\r\n",
        "epochs = 10 # The number of epochs\r\n",
        "best_model = None\r\n",
        "epoch_losses = np.array([])\r\n",
        "epoch = 1\r\n",
        "\r\n",
        "if os.path.isfile(F\"{DRIVE_FOLDER}/{model_save_name}\" ):\r\n",
        "    print('loading checkpoint...')\r\n",
        "    checkpoint = torch.load(MODEL_PATH)\r\n",
        "    epoch = checkpoint['epoch']\r\n",
        "    saved_loss = checkpoint['loss']\r\n",
        "    \r\n",
        "    if saved_loss < best_val_loss:\r\n",
        "        best_val_loss = saved_loss\r\n",
        "\r\n",
        "while epoch < epochs + 1:\r\n",
        "    epoch_start_time = time.time()\r\n",
        "    train()\r\n",
        "    val_loss = evaluate(model, val_data)\r\n",
        "    epoch_losses = np.append(epoch_losses, val_loss)\r\n",
        "    print('-' * 89)\r\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\r\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\r\n",
        "                                     val_loss, math.exp(val_loss)))\r\n",
        "    print('-' * 89)\r\n",
        "\r\n",
        "    if val_loss < best_val_loss:\r\n",
        "        best_val_loss = val_loss\r\n",
        "        best_model = model\r\n",
        "        torch.save({\r\n",
        "            'epoch': epoch,\r\n",
        "            'model_state_dict': model.state_dict(),\r\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "            'loss': val_loss,\r\n",
        "        }, MODEL_PATH)\r\n",
        "\r\n",
        "    epoch = epoch + 1\r\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdAQOSI06UWM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}